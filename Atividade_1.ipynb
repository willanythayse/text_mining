{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Atividade_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/willanythayse/text_mining/blob/master/Atividade_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "TzaMB5MgU-Xt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Library"
      ]
    },
    {
      "metadata": {
        "id": "IxZCl7V1VJ7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c9996443-2544-4546-a30a-66f0f4d2fe11"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "oO_iqO1kX4h9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Variáveis de Entrada "
      ]
    },
    {
      "metadata": {
        "id": "JnVCK9hnX8N0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Text1 = \"Olha, lamentavelmente, a crise elétrica já dura uma década na Venezuela. Hugo Chávez decretou uma emergência elérica em 2009, resultado do que chamaram de uma crise gerada pela natureza. Naquele momento, foi o El Niño. Investiram US$ 100 bilhões (R$ 384,7 bilhões) no sistema elétrico venezuelano. Em 2013, o sistema elétrico foi militarizado. Agora, culparam uma iguana, que teria comido um cabo e produzido o apagão, na versão oficial.\"\n",
        "#Text2 = \"Se a situação não fosse trágica na Venezuela, se não fosse lastimável e não agravasse a complexa crise humanitária que vive o país, eu daria risada. Mas não se pode rir quando morrem crianças, quando passamos seis dias sem poder trabalhar no país, quando isso impacta desta forma nosso cotidiano e a economia.\"\n",
        "\n",
        "Text1 = \"A B C\"\n",
        "Text2 = \"E D A B\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oDkbaMXhdK62",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tokenização"
      ]
    },
    {
      "metadata": {
        "id": "8vl_GIi1dKOa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ac6e6491-e041-4d28-a4e1-ad7701800889"
      },
      "cell_type": "code",
      "source": [
        "tokenized_text1=sent_tokenize(Text1)\n",
        "print(tokenized_text1)\n",
        "\n",
        "tokenized_text2=sent_tokenize(Text2)\n",
        "print(tokenized_text2)\n",
        "\n",
        "tokenized_word1=word_tokenize(Text1)\n",
        "print(tokenized_word1)\n",
        "\n",
        "tokenized_word2=word_tokenize(Text2)\n",
        "print(tokenized_word2)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A B C']\n",
            "['E D A B']\n",
            "['A', 'B', 'C']\n",
            "['E', 'D', 'A', 'B']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2s33QsdMf4Dw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# StopWords"
      ]
    },
    {
      "metadata": {
        "id": "TKXAuO_qf5mF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c1a99eae-364b-4691-f6d0-7af2068f2692"
      },
      "cell_type": "code",
      "source": [
        "# stopWords = set(stopwords.words('english'))\n",
        "# stopWords = set(stopwords.words('portuguese'))\n",
        "\n",
        "stopWords = \"B\"\n",
        "wordsFiltered = []\n",
        " \n",
        "for w in tokenized_word1:\n",
        "    if w not in stopWords:\n",
        "        wordsFiltered.append(w)\n",
        "        \n",
        "for y in tokenized_word2:\n",
        "    if y not in stopWords:\n",
        "        wordsFiltered.append(y)\n",
        "\n",
        "\n",
        "Text1_token = wordsFiltered[0] + wordsFiltered[1]\n",
        "Text2_token = wordsFiltered[2] + wordsFiltered[3] + wordsFiltered[4]\n",
        "\n",
        "print(tokenized_word1)\n",
        "print(tokenized_word2)\n",
        "print(stopWords)\n",
        "print(wordsFiltered)\n",
        "#print(Text1_token)\n",
        "#print(Text2_token)\n",
        "\n",
        "count=0\n",
        "soma=0\n",
        "for associacao in Text1_token:\n",
        "  #print(associacao)\n",
        "  for ass in Text2_token:\n",
        "    #print(associacao) \n",
        "    if associacao not in stopWords:\n",
        "      wordsFiltered.append(y)\n",
        "    word = associacao+ass\n",
        "    count = count+1\n",
        "    #print(ass)\n",
        "    print(word)\n",
        "\n",
        "\n",
        "print(count)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A', 'B', 'C']\n",
            "['E', 'D', 'A', 'B']\n",
            "B\n",
            "['A', 'C', 'E', 'D', 'A']\n",
            "AE\n",
            "AD\n",
            "AA\n",
            "CE\n",
            "CD\n",
            "CA\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nsg3JJ0Ve_SS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Associação entre Palavras"
      ]
    },
    {
      "metadata": {
        "id": "tFQxhMbBfCRi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for associacao in Text1:\n",
        "  for \n",
        "  \n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}