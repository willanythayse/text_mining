---
title: "Text Mining"
subtitle: "Análise de Sentimento"
author: "Prof.Gustavo Mirapalheta"
output:
  ioslides_presentation:
    incremental: no
    mouse_click_enabled: yes
    widescreen: yes
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(dplyr)
library(janeaustenr)
setwd("../scripts")
```

## Análise de Sentimento

![O processo de Análise de Sentimento  \label{fig002}](../imagens/textmining002.png)

- Considera-se o texto como a combinação de suas palavras individuais e o conteúdo de sentimento do texto como a soma do sentimento de suas palavras individuais.

## Análise de Sentimento: dataset sentiments

- Conjunto de léxicos com a classificação das palavras de acordo com um sentimento específico ou um nível associado

```{r}
head(tidytext::sentiments)
```

## Análise de Sentimento: léxicos disponíveis

- AFINN (Finn Årup Nielsen)
    + por escore (-5 a 5)
- Bing (Bing Liu and collaborators)
    + binária ("positiva","negativa") 
- NRC (Saif Mohammad and Peter Turney)
    + binária ("positiva","negativa") e nas categorias: positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, e trust
- loughran
    + categorias: negative, positive, uncertainty,  litigious,   
constraining, superfluous
    
## Análise de Sentimento: exemplos de léxicos

- NRC (Saif Mohammad and Peter Turney)
    + binária ("positiva","negativa") e nas categorias: positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, e trust

```{r}
get_sentiments("nrc") %>% head()
```

## Análise de Sentimento: exemplos de léxicos

- Bing (Bing Liu and collaborators)
    + binária ("positiva","negativa")

```{r}
get_sentiments("bing") %>% head()
```

## Análise de Sentimento: exemplos de léxicos

- AFINN (Finn Årup Nielsen)
    + por escore (-5 a 5)

```{r}
get_sentiments("afinn") %>% head()
```

## Análise de Sentimento: exemplos de léxicos

- loughran
    + categorias: negative, positive, uncertainty,  litigious,   
constraining, superfluous

```{r}
get_sentiments("loughran") %>% head()
```

## Análise de Sentimento com dados "Tidy"

- Assim como a remoção dos *stop words* era uma operação *anti_join* a análise de sentimento é um *inner_join*.

- Por exemplo, vamos determinar quais as palavras de *joy* mais comuns em *Emma*, tomando por base o léxico NRC.

- Primeiro temos que baixar o texto de *Emma* e torna-lo *tidy* através de *unnest_tokens()* e da inclusão das colunas referentes à linha e ao capítulo de origem

## Análise de Sentimento: *joy* em *Emma*

- Os livros estão disponíveis no pacote *janeaustenr*, através da função *austen_books()*. O essencial para o exercício que iremos realizar seria a coluna de text. No entanto, vamos também criar colunas indicando o número do capítulo e da linha correspondente, para efeito de prática nos recursos de mineração de texto, como pode ser visto no próximo slide. 

## Análise de Sentimento: *joy* em *Emma*

- Para incluir os números de capítulo e linha divididos por livro, iniciamos agrupando os textos pela campo *book*.

```{r}
group_by(austen_books(), book) -> austen_group; austen_group
```

## Análise de Sentimento: *joy* em *Emma*

- Em seguida inserimos o número de linha, no dataframe agrupado por *book*.

```{r}
mutate(austen_group, linenumber = row_number()) -> austen_mutate1; austen_mutate1
```

## Análise de Sentimento: *joy* em *Emma*

- E depois o número do capítulo com a regex: "^chapter [\\divxlc]"

```{r}
mutate(austen_mutate1, chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
            ignore_case = TRUE) ) ) ) -> austen_mutate2; austen_mutate2
```

## Análise de Sentimento: *joy* em *Emma*

- Temos agora um dataframe no qual além do texto e do livro, linha por linha, temos dois campos, um com o número da linha e outro com o número do capítulo associado. Podemos agora prosseguir para a segunda parte do exercício, isto é a tokenização do mesmo por palavra (vide slide a seguir)

## Análise de Sentimento: *joy* em *Emma*

- Desagrupamos o dataframe para que seja feita a tokenização por palavra

```{r}
ungroup(austen_mutate2) -> austen_ungroup; head(austen_ungroup)
```

## Análise de Sentimento: *joy* em *Emma*

- Em seguida tokenizamos o dataframe, uma palavra por linha

```{r}
unnest_tokens(austen_ungroup, word, text) -> austen_unnest; head(austen_unnest)
```

## Análise de Sentimento: *joy* em *Emma*

- Observe no slide anterior que escolhemos o nome "word" para a coluna de resultado de unnest
    + Isto torna os *inner_joins* mais fáceis de executar pois os datasets de *stop words* e de *léxicos* já possuem uma coluna com nome igual
    
## Análise de Sentimento: *joy* em *Emma*

- Precisamos agora eliminar os caracteres "_" das palavras, pois os mesmos indicam que o texto original estava em itálico, de acordo com as regras do Projeto Gutemberg. Fazemos isto para evitar que existam tokens como "\_you\_"

- Para isso lançamos mão da regex: "[a-z']+". Esta regex vai eliminar também números dos tokens, mas isto seria feito de qualquer forma depois pelo inner_join com os léxicos, os quais contém apenas palavras.

- A operação poderá ser vista no próximo slide.
    
## Análise de Sentimento: *joy* em *Emma*

```{r}
mutate(austen_unnest, word = str_extract(word,
                                         "[a-z']+")) -> austen_unnest; austen_unnest
```

## Análise de Sentimento: *joy* em *Emma*

- Por precaução vamos filtrar também os NAs
    
```{r}
filter(austen_unnest, !is.na(word)) -> austen_unnest; austen_unnest
```

## Análise de Sentimento: *joy* em *Emma*

- Primeiro filtramos o léxico "nrc" para *joy*

```{r}
nrcjoy <- filter(get_sentiments("nrc"),sentiment=="joy"); nrcjoy
```

## Análise de Sentimento: *joy* em *Emma*

- Depois filtramos o dataframe *austen_unnest* para *Emma*

```{r}
books_emma <- filter(austen_unnest, book=="Emma"); books_emma
```

## Análise de Sentimento: *joy* em *Emma*

- Em seguida fazemos o *inner_join* dos dois

```{r, message=FALSE}
emma_joy <- inner_join(books_emma, nrcjoy, by=c("word"="word")); emma_joy
```

## Análise de Sentimento: *joy* em *Emma*

- E por último calculamos a contagem de palavras

```{r}
emma_count <- count(emma_joy, word, sort=TRUE); emma_count
```

## Análise de Sentimento em Jane Austen

- Para tal vamos criar um escore de sentimento através do léxico *bing* e da função *inner_join*

- Dividiremos os textos em grupos de 80 linhas, através de um índice calculado por *x %/% y*, o qual executa divisão inteira. 
    + Por que 80? Poucas linhas podem não produzir uma estimativa significativa. Muitas linhas podem "limpar" a estrutura do texto. O valor 80 depende do texto. No caso de Jane Austen, isto produz resultados interessantes.
    
- Utilizamos em seguida *spread* para ter os valores positivos e negativos em colunas separadas. Calculamos em seguida o valor líquido fazendo *positivas* - *negativas* para cada seção de 80 linhas do texto.

## Análise de Sentimento em Jane Austin

- Primeiro cruzamos *austen_unnest* com o léxico *bing*

```{r, message=FALSE}
inner_join(austen_unnest, get_sentiments("bing")) -> tidy_join; tidy_join
```

## Análise de Sentimento em Jane Austin

- Depois criamos um índice que irá indicar o número do grupo de 80 linhas no qual a palavra se encontra

```{r, message=FALSE}
count(tidy_join, book, 
      index = linenumber %/% 80, sentiment) -> tidy_count; tidy_count
```

## Análise de Sentimento em Jane Austin

- Utilizando a função *spread* dividimos a coluna *sentiment* em duas, uma com valores positivos e outra com valores negativos (de *joy*). 

```{r, message=FALSE}
spread(tidy_count, sentiment, 
       n, fill = 0) -> tidy_spread; tidy_spread
```

## Análise de Sentimento em Jane Austin

- Criamos em seguida uma coluna *sentiment* com o valor líquido entre a soma dos valores positivos e negativos de sentimento

```{r, message=FALSE}
mutate(tidy_spread, 
       sentiment = positive - negative) -> tidy_mutate; tidy_mutate
```

## Análise de Sentimento em Jane Austin

- E por último renomeanos o dataframe para *janeaustensentiment*

```{r, message=FALSE}
tidy_mutate -> janeaustensentiment; janeaustensentiment
```

## Análise de Sentimento em Jane Austin

- Vamos agora plotar o gráfico da alteração de sentimento, ao longo de cada um dos romances de Jane Austen
    + Observe nos slides a seguir que o índice que controla o avanço de cada romance é o número do grupo de 80 linhas
    
## Análise de Sentimento em Jane Austin

- Primeiro criamos o mapeamento estético básico do ggplot2     
    
```{r}
ggplot(janeaustensentiment, 
       aes(index, sentiment, fill = book)) -> jane_plot; jane_plot
```

## Análise de Sentimento em Jane Austin

- Depois adicionamos um gráfico de barras, porém sobreposto

```{r}
jane_plot + geom_col(show.legend = FALSE) -> jane_col; jane_col
``` 

## Análise de Sentimento em Jane Austin

- E para melhorar a visualização, dividimos o mesmo por *book*

```{r}
jane_col + facet_wrap(~book, ncol = 2, scales = "free_x") 
```

## Análise Comp.de Léxicos em *Pride and Prejudice*

- Vamos executar agora a mesma análise, porém com diferentes léxicos, na obra *Pride and Prejudice* para determinar o nível de influência do léxico na análise.

- Obs: o índice de AFINN terá de ser calculado de forma distinta pois na forma bruta é um escore de -5 a +5 

## Análise Comp.de Léxicos em *Pride and Prejudice*

- Primeiro filtramos o dataframe austen_unnest para *Pride and Prejudice*

```{r}
filter(austen_unnest, book=="Pride & Prejudice")->pride_prejudice;  pride_prejudice
```

## Análise Comp.de Léxicos em *Pride and Prejudice* 

- Vamos agora executar a mesma análise, com o léxico *Afinn*

```{r, message=FALSE}
inner_join(pride_prejudice, get_sentiments("afinn"))->inner_afinn; inner_afinn
```

## Análise Comp.de Léxicos em *Pride and Prejudice* 

- Em seguida agrupamos por um campo denominado *index* que determina o número do grupo de 80 linhas a que o token (palavra) pertence.

```{r}
group_by(inner_afinn, index = linenumber %/% 80) -> inner_group; inner_group
```

## Análise Comp.de Léxicos em *Pride and Prejudice* 

- Calculamos a soma de *index* por grupo de 80 linhas(pois inner_group) foi agrupado justamente por *index* em um campo denominado *sentiment*. 
```{r}
summarise(inner_group, sentiment = sum(score))-> inner_sum; inner_sum
```

## Análise Comp.de Léxicos em *Pride and Prejudice* 

- E inserimos um campo denominado *method* para indicar que o sentimento líquido do grupo de 80 linhas foi calculado pelo léxico *AFINN*

```{r}
mutate(inner_sum, method="AFINN") -> affin; affin
```

## Análise Comp.de Léxicos em *Pride and Prejudice*

- A análise toda poderia ser feita de maneira direta com o uso de pipes (%>%)

```{r, message=FALSE}
afinn <- pride_prejudice %>% inner_join(get_sentiments("afinn")) %>%
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(score)) %>% mutate(method = "AFINN"); afinn
```

## Análise Comp.de Léxicos em *Pride and Prejudice*

- Vamos realizar uma análise similar com o léxico *bing*

```{r}
pride_prejudice %>% inner_join(get_sentiments("bing")) %>%
mutate(method = "Bing et al.") -> inner_bing; inner_bing
```

## Análise Comp.de Léxicos em *Pride and Prejudice*

- E outra análise similar com o léxico *nrc*

```{r}
pride_prejudice %>% inner_join(get_sentiments("nrc") %>%
filter(sentiment %in% c("positive", "negative"))) %>%
mutate(method = "NRC")->inner_nrc; inner_nrc
```

## Análise Comp.de Léxicos em *Pride and Prejudice*

- Em seguida vamos juntar por linhas os dois dataframes

```{r}
bind_bing_and_nrc <- bind_rows(inner_bing, inner_nrc); bind_bing_and_nrc
```

## Análise Comp.de Léxicos em *Pride and Prejudice*

- Depois devemos determinar o grupo de 80 linhas a que cada palavra se refere

```{r}
bind_count <- count(bind_bing_and_nrc, method, 
                    index = linenumber %/% 80, sentiment); bind_count
```

## Análise Comp.de Léxicos em *Pride and Prejudice*

- Dividir o campo *sentiment* em colunas de positivos e negativos

```{r}
bind_spread <- spread(bind_count, sentiment, n, fill = 0); bind_spread
```

## Análise Comp.de Léxicos em *Pride and Prejudice*

- E calcular o valor líquido do mesmo

```{r}
bing_and_nrc  <- mutate(bind_spread, sentiment = positive - negative); bing_and_nrc  
```

## Análise Comp.de Léxicos em *Pride and Prejudice*

- Unimos em seguida os dataframes com os três léxicos

```{r}
bind_rows(afinn, bing_and_nrc) -> lexicos; lexicos
```

## Análise Comp.de Léxicos em *Pride and Prejudice*

```{r}
ggplot(lexicos, aes(index, sentiment, fill = method)) + geom_col(show.legend = FALSE) +
facet_wrap(~method, ncol = 1, scales = "free_y")
```

## Análise Comparativa de Léxicos: relação w(+)/w(-)

```{r}
get_sentiments("nrc")
```

## Análise Comparativa de Léxicos: relação w(+)/w(-)

```{r}
get_sentiments("nrc") %>% filter(sentiment %in% c("positive", "negative"))
```

## Análise Comparativa de Léxicos: relação w(+)/w(-)

```{r}
get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)
```

## Análise Comparativa de Léxicos: relação w(+)/w(-)

```{r}
get_sentiments("bing")
```

## Análise Comparativa de Léxicos: relação w(+)/w(-)

```{r}
get_sentiments("bing") %>% count(sentiment)
```

## Palavras (+) e (-) mais comuns nos Léxicos

```{r, message=FALSE}
bing_word_counts <- austen_unnest %>% inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>% ungroup(); bing_word_counts
```

## Palavras (+) e (-) mais comuns nos Léxicos

```{r, message=FALSE}
bing_word_counts %>% group_by(sentiment) %>% top_n(10) %>% ungroup() %>% 
mutate(word = reorder(word, n)) %>% ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment", x = NULL) + coord_flip() -> bing_ggplot
```

## Palavras (+) e (-) mais comuns nos Léxicos

```{r}
print(bing_ggplot)
```

## Wordclouds

- Vamos primeiro preparar o tibble para apresentação em forma de wordcloud. Começamos com austen_unnest, o qual como pode ser visto abaixo, contém as palavras nos livros de Jane Austen divididas por livro ("book"), capítulo ("chapter") e número de linha ("linenumber").

```{r}
austen_unnest
```

## Wordclouds

- Em seguida criamos dataframes tokenizados de forma similar para as Irmães Bronte.

```{r}
#bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))
bronte <- read.csv2("../dados/bronte.csv", 
                    stringsAsFactors=FALSE)
tidy_bronte <- bronte %>%
  unnest_tokens(word, text) %>%
  mutate(word = str_extract(word, "[a-z']+")) %>%
  anti_join(stop_words, by=c("word"="word"))
```

## Wordclouds (cont.)

- Fazemos o mesmo para o dataframe com as obras de H.G.Wells

```{r}
#hgwells <- gutenberg_download(c(35, 36, 5230, 159))
hgwells <- read.csv2("../dados/hgwells.csv", 
                     stringsAsFactors=FALSE)
tidy_hgwells <- hgwells %>%
  unnest_tokens(word, text) %>%
  mutate(word = str_extract(word, "[a-z']+")) %>%
  anti_join(stop_words)
```

## Wordclouds (cont.)

- E mais uma vez unimos os dataframes por linha

```{r}
mutate(tidy_bronte, author = "Brontë Sisters")->tidy_bronte2
mutate(tidy_hgwells, author = "H.G. Wells")->tidy_hgwells2
mutate(austen_unnest, author = "Jane Austen")->tidy_books2
tidy_books2
```

## Wordclouds - Palavras + comuns em Jane Austen

```{r, message=FALSE, warning=FALSE}
library(wordcloud); anti_join(tidy_books2, stop_words) -> books_antijoin
count(books_antijoin, word) -> books_count
with(books_count, wordcloud(word, n, max.words=100)) -> books_cloud
```

## Wordcloud & Sentiment - Jane Austen Novels

- Vamos agora realizar uma análise de sentimento, determinando as palavras positivas e negativas mais comuns. Iniciamos gerando o tibble correspondente.

```{r, message=FALSE, warning=FALSE}
library(reshape2)
austen_unnest %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) -> tidy_comparison
```

## Wordcloud & Sentiment - Jane Austen Novels

- O tibble que será usado para gerar a nuvem comparativa é apresentado a seguir. 

```{r}
head(tidy_comparison)
```

## Wordcloud & Sentiment - Jane Austen Novels

- Obs: os tamanhos não são comparáveis entre sentimentos diferentes

```{r}
comparison.cloud(tidy_comparison, colors = c("red", "blue"), max.words = 100)
```

## Análise de Sentimentos em Frases

- "I am not having a good day" 
  + Sentença negativa por causa do "not"
  
- Pacotes para análise de sentenças: *coreNLP*, *cleanNLP* e *sentimentr*

- Tokenização por sentença no tibble

```{r}
PandP_sentences <- data_frame(text = prideprejudice) %>%
unnest_tokens(sentence, text, token = "sentences")
```

## Análise de Sentimentos em Frases - exemplo de tibble

- Observe o tamanho da sentença quando o arquivo está em formato UTF-8

```{r}
PandP_sentences$sentence[2]
```

## Análise de Sentimentos em Capítulos - exemplo de tibble

- Vamos dividir os tokens através de uma *regex*. No exemplo abaixo cada token será um capítulo. Vamos primeiro determinar se a *regex* é capaz de contar corretamente os capítulos de cada livro.

- O tibble pode ser visto no próximo slide

```{r}
austen_chapters <- austen_books() %>%
group_by(book) %>%
unnest_tokens(chapter, text, token = "regex",
pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
ungroup()
```

## Análise de Sentimentos em Capítulos - exemplo de tibble

```{r}
austen_chapters %>%
group_by(book) %>%
summarise(chapters = n())
```

## Análise de Sentimentos por Capítulo

- Nosso objetivo será determinar qual capítulo possui a maior proporção de palavras negativas.

## Análise de Sentimentos por Capítulo

- Vamos obter a lista de palavras negativas no Bing.

```{r}
bingnegative <- get_sentiments("bing") %>%
filter(sentiment == "negative")
head(bingnegative)
```

## Análise de Sentimentos por Capítulo

- Em seguida vamos criar um data frame com quantas palavras estão em cada capítulo de forma a normalizar os resultados. 

```{r}
wordcounts <- austen_unnest %>%
group_by(book, chapter) %>%
summarize(words = n())
head(wordcounts)
```

## Análise de Sentimentos por Capítulo

- Depois vamos calcular o número de palavras negativas em cada capítulo e dividir pelo total de palavras por capítulo. 

```{r, warning=FALSE, message=FALSE}
austen_unnest %>% semi_join(bingnegative) %>%
group_by(book, chapter) %>% summarize(negativewords = n()) %>%
left_join(wordcounts, by = c("book", "chapter")) %>%
mutate(ratio = negativewords/words) %>% filter(chapter != 0) %>%
top_n(1) %>% ungroup()-> tidy_sentiment_chapter; 
head(tidy_sentiment_chapter)
```

## Análise de Sentimentos por Capítulo

- O que acontece nos capítulos listados?
   + Sense and Sensibility (43): Marianne está gravemente doente, perto da morte.
   + Pride and Prejudice (34): Sr. Darcy propõe pela primeira vez...     + Mansfield Park (46): quase no fim, quando todos descobrem o adultério escandaloso de Henry
   + Emma (15): O horripilante o Sr. Elton propõe
   + Northanger Abbey (21): Catherine está no fundo de sua fantasia gótica de assassinato. 
   + Persuasion (4): o leitor conhece a recusa do Capitão Wentworth por Anne (em flashback) e ela percebe o erro que cometeu.
   
   